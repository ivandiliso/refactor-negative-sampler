{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3936bbe",
   "metadata": {},
   "source": [
    "# PyKEEN Negative Sampling Extension: A introductory Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c7e9d",
   "metadata": {},
   "source": [
    "This tutorial will guide you with the basic usage of the new negative samplers classes. Remember to unzip the provided dataset file the the data/ folder, in order to assure the correct functionalities of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec0851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pykeen\n",
    "import torch\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.sampling import (\n",
    "    BasicNegativeSampler,\n",
    "    BernoulliNegativeSampler,\n",
    "    negative_sampler_resolver,\n",
    ")\n",
    "from pykeen.sampling.filtering import filterer_resolver\n",
    "from pykeen.training import SLCWATrainingLoop\n",
    "\n",
    "from extension.dataset import OnMemoryDataset\n",
    "from extension.filtering import NullPythonSetFilterer\n",
    "from extension.sampling import (\n",
    "    CorruptNegativeSampler,\n",
    "    NearestNeighbourNegativeSampler,\n",
    "    NearMissNegativeSampler,\n",
    "    RelationalNegativeSampler,\n",
    "    SubSetNegativeSampler,\n",
    "    TypedNegativeSampler,\n",
    ")\n",
    "import extension.constants as const"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb0658",
   "metadata": {},
   "source": [
    "This should automatically get the correct data path given the tutorial provided location, if needed, modify this path with your custom \"data\" folder path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8857b6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/navis/dev/refactor-negative-sampler/data')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path().cwd().parent / \"data\"\n",
    "data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b02245",
   "metadata": {},
   "source": [
    "## Loading the provided datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97aa95",
   "metadata": {},
   "source": [
    "Let's load the YAGO4-20 dataset, with the additional provided metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20081d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OnMemoryDataset(\n",
    "    data_path=data_path / \"YAGO4-20\", load_domain_range=True, load_entity_classes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f625999",
   "metadata": {},
   "source": [
    "Now you can use all the basic functionalities of the standard pykeen dataset, with added loaded data of domain and range proprieties, and entity class membership, lets see them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89adfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Entities 96910\n",
      "Num Relations 70\n",
      "Relation Mapping {'about': 0, 'actor': 1, 'affiliation': 2, 'alumniOf': 3, 'author': 4, 'award': 5, 'bioChemInteraction': 6, 'birthPlace': 7, 'brand': 8, 'byArtist': 9, 'character': 10, 'children': 11, 'citation': 12, 'competitor': 13, 'composer': 14, 'containedInPlace': 15, 'containsPlace': 16, 'contentLocation': 17, 'contributor': 18, 'copyrightHolder': 19, 'countryOfOrigin': 20, 'creator': 21, 'deathPlace': 22, 'director': 23, 'editor': 24, 'exampleOfWork': 25, 'familyName': 26, 'founder': 27, 'foundingLocation': 28, 'gender': 29, 'genre': 30, 'givenName': 31, 'hasMolecularFunction': 32, 'hasOccupation': 33, 'hasPart': 34, 'homeLocation': 35, 'honorificPrefix': 36, 'inLanguage': 37, 'isBasedOn': 38, 'isInvolvedInBiologicalProcess': 39, 'isLocatedInSubcellularLocation': 40, 'isPartOf': 41, 'knowsLanguage': 42, 'license': 43, 'location': 44, 'locationCreated': 45, 'material': 46, 'memberOf': 47, 'musicBy': 48, 'nationality': 49, 'parent': 50, 'parentOrganization': 51, 'parentTaxon': 52, 'partOfSeason': 53, 'partOfSeries': 54, 'producer': 55, 'productionCompany': 56, 'publisher': 57, 'relevantSpecialty': 58, 'signOrSymptom': 59, 'sponsor': 60, 'sport': 61, 'spouse': 62, 'subEvent': 63, 'subOrganization': 64, 'superEvent': 65, 'taxonRank': 66, 'translator': 67, 'workExample': 68, 'worksFor': 69}\n",
      "Domain and Range of 'character': {'domain': 'CreativeWork', 'range': 'Person'}\n",
      "Entity Classes of '10_Rillington_Place': ['Movie']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num Entities {dataset.num_entities}\")\n",
    "print(f\"Num Relations {dataset.num_relations}\")\n",
    "print(f\"Relation Mapping {dataset.relation_to_id}\")\n",
    "\n",
    "relation_id = 10\n",
    "entity_id = 50\n",
    "\n",
    "id_to_entity = {v: k for k, v in dataset.entity_to_id.items()}\n",
    "id_to_relation = {v: k for k, v in dataset.relation_to_id.items()}\n",
    "\n",
    "print(\n",
    "    f\"Domain and Range of '{id_to_relation[relation_id]}': {dataset.relation_id_to_domain_range[relation_id]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Entity Classes of '{id_to_entity[entity_id]}': {dataset.entity_id_to_classes[entity_id]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399b8e0",
   "metadata": {},
   "source": [
    "## Using the static negative samplers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45daf0b7",
   "metadata": {},
   "source": [
    "Lets instantiate some static negative samplers, the corrupt, and typed  that used the additional metadata and relationa. We just use the pykeen inferface and provide the additional required metadata loaded with the dataset. In this case we set the integration of random negatives to false, in oder to showcase the real negatives generated with these methods. Since relational uses a heavy filtering criterion, it will create a cached file to store the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51811e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RelationalNegativeSampler] Loading Pre-Computed Subset\n"
     ]
    }
   ],
   "source": [
    "filterer = NullPythonSetFilterer(mapped_triples=dataset.training.mapped_triples)\n",
    "\n",
    "samplers = {\n",
    "    \"Corrupt\": CorruptNegativeSampler(\n",
    "        mapped_triples=dataset.training.mapped_triples,\n",
    "        filtered=True,\n",
    "        filterer=filterer,\n",
    "        num_negs_per_pos=5,\n",
    "        integrate=False,\n",
    "    ),\n",
    "    \"Typed\": TypedNegativeSampler(\n",
    "        mapped_triples=dataset.training.mapped_triples,\n",
    "        filtered=True,\n",
    "        filterer=filterer,\n",
    "        num_negs_per_pos=5,\n",
    "        entity_classes_dict=dataset.entity_id_to_classes,\n",
    "        relation_domain_range_dict=dataset.relation_id_to_domain_range,\n",
    "        integrate=False,\n",
    "    ),\n",
    "    \"Relational\": RelationalNegativeSampler(\n",
    "        mapped_triples=dataset.training.mapped_triples,\n",
    "        filtered=True,\n",
    "        filterer=filterer,\n",
    "        num_negs_per_pos=5,\n",
    "        local_file=\"relational_cached.bin\",\n",
    "        integrate=False,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824dc21",
   "metadata": {},
   "source": [
    "Now lets use the sampler to produce the negative for the first 2 triples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf7e819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Sampler: Corrupt\n",
      "(tensor([[[    0,     1, 49067],\n",
      "         [63642,     1,  5226],\n",
      "         [    0,     1, 81587],\n",
      "         [    0,     1, 17358],\n",
      "         [11297,     1,  5226]],\n",
      "\n",
      "        [[    0,     1, 63254],\n",
      "         [    0,     1, 52264],\n",
      "         [ 3889,     1, 19014],\n",
      "         [31922,     1, 19014],\n",
      "         [    0,     1, 56737]]]), tensor([[True, True, True, True, True],\n",
      "        [True, True, True, True, True]]))\n",
      "\n",
      "Negative Sampler: Typed\n",
      "(tensor([[[    0,     1,  1717],\n",
      "         [10968,     1,  5226],\n",
      "         [    0,     1, 73238],\n",
      "         [    0,     1,  1717],\n",
      "         [59817,     1,  5226]],\n",
      "\n",
      "        [[72558,     1, 19014],\n",
      "         [    0,     1,  1717],\n",
      "         [92845,     1, 19014],\n",
      "         [    0,     1,  1717],\n",
      "         [    0,     1, 43149]]]), tensor([[True, True, True, True, True],\n",
      "        [True, True, True, True, True]]))\n",
      "\n",
      "Negative Sampler: Relational\n",
      "(tensor([[[   -1,     1,  5226],\n",
      "         [    0,     1, 74377],\n",
      "         [    0,     1, 74377],\n",
      "         [   -1,     1,  5226],\n",
      "         [    0,     1, 72454]],\n",
      "\n",
      "        [[    0,     1, 74377],\n",
      "         [   -1,     1, 19014],\n",
      "         [    0,     1, 72454],\n",
      "         [    0,     1, 42613],\n",
      "         [   -1,     1, 19014]]]), tensor([[False,  True,  True, False, False],\n",
      "        [ True, False, False,  True, False]]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, sampler in samplers.items():\n",
    "    print(f\"Negative Sampler: {name}\")\n",
    "    print(samplers[name].sample(dataset.training.mapped_triples[:2]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655147c8",
   "metadata": {},
   "source": [
    "You can see that some triples have negative index entities, this are placeholders filterred by the NullPythonSetFilterer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b501b6c",
   "metadata": {},
   "source": [
    "## Using dynamic negative samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dc2ba6",
   "metadata": {},
   "source": [
    "In oder to use the dynamic negative samplers, we will need to first pretrain a model, for this purpose, lets train Transe on YAGO for 2 epochs, just for the sake of the tutorial, using a random sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c4583b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No random seed is specified. This may lead to non-reproducible results.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4324b7ae8dac4a339b08a930e4d97020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cpu:   0%|          | 0/2 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc52b3a23d92435fb21c61264b6f171f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/2169 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7572c03f2434096b7c2e6e127992a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/2169 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.9830639038343153, 0.8069588712087256]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pykeen.models\n",
    "\n",
    "\n",
    "model = pykeen.models.TransE(triples_factory=dataset.training, embedding_dim=10)\n",
    "\n",
    "loop = SLCWATrainingLoop(\n",
    "    model=model, triples_factory=dataset.training, optimizer=\"Adam\"\n",
    ")\n",
    "\n",
    "loop.train(triples_factory=dataset.training, num_epochs=2, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e932f",
   "metadata": {},
   "source": [
    "Now lets define the custom function used for prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8898f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_model_prediction(model, hrt_batch, targets):\n",
    "    out = torch.zeros(\n",
    "        (hrt_batch.size(0), model.entity_representations[0]().size(1)),\n",
    "        device=hrt_batch.device,\n",
    "    )\n",
    "    out[targets == 0] = model.entity_representations[0](\n",
    "        hrt_batch[targets == 0, 2]\n",
    "    ) - model.relation_representations[0](hrt_batch[targets == 0, 1])\n",
    "    out[targets == 2] = model.entity_representations[0](\n",
    "        hrt_batch[targets == 2, 0]\n",
    "    ) + model.relation_representations[0](hrt_batch[targets == 2, 1])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1afe5",
   "metadata": {},
   "source": [
    "And not we can instantiate the Adversarial negative sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7778268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Negative Sampler\n",
      "[\u001b[92mDONE\u001b[0m ] [NS NearMissNegativeSampler] Calculating HEAD prediction with TransE pretrained model in \u001b[96m0000.0007\u001b[0ms\n",
      "[\u001b[92mDONE\u001b[0m ] [NS NearMissNegativeSampler] Calculating TAIL prediction with TransE pretrained model in \u001b[96m0000.0002\u001b[0ms\n",
      "[\u001b[92mDONE\u001b[0m ] [NS NearMissNegativeSampler] Querying KDTREE for HEAD predictions in \u001b[96m0000.0010\u001b[0ms\n",
      "[\u001b[92mDONE\u001b[0m ] [NS NearMissNegativeSampler] Querying KDTREE for TAIL predictions in \u001b[96m0000.0009\u001b[0ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 667.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[94881,     1,  5226],\n",
      "         [65931,     1,  5226],\n",
      "         [    0,     1, 68207],\n",
      "         [    0,     1, 37774],\n",
      "         [    0,     1, 66097]],\n",
      "\n",
      "        [[15681,     1, 19014],\n",
      "         [79459,     1, 19014],\n",
      "         [    0,     1, 37774],\n",
      "         [    0,     1, 26074],\n",
      "         [    0,     1, 66097]]]), tensor([[True, True, True, True, True],\n",
      "        [True, True, True, True, True]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = NearMissNegativeSampler(\n",
    "    mapped_triples=dataset.training.mapped_triples,\n",
    "    prediction_function=sampling_model_prediction,\n",
    "    sampling_model=model,\n",
    "    num_negs_per_pos=5,\n",
    "    num_query_results=10,\n",
    "    filtered=True,\n",
    "    filterer=filterer,\n",
    ")\n",
    "\n",
    "print(\"Adversarial Negative Sampler\")\n",
    "print(sampler.sample(dataset.training.mapped_triples[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcfee31",
   "metadata": {},
   "source": [
    "Or the nearest neighbour negative sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380497b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Negative Sampler\n",
      "(tensor([[[    0,     1, 50626],\n",
      "         [    0,     1, 41365],\n",
      "         [28108,     1,  5226],\n",
      "         [52837,     1,  5226],\n",
      "         [    0,     1, 41365]],\n",
      "\n",
      "        [[52837,     1, 19014],\n",
      "         [    0,     1, 14871],\n",
      "         [    0,     1, 81824],\n",
      "         [82036,     1, 19014],\n",
      "         [    0,     1,  4414]]]), tensor([[True, True, True, True, True],\n",
      "        [True, True, True, True, True]]))\n"
     ]
    }
   ],
   "source": [
    "sampler = NearestNeighbourNegativeSampler(\n",
    "    mapped_triples=dataset.training.mapped_triples,\n",
    "    sampling_model=model,\n",
    "    num_negs_per_pos=5,\n",
    "    num_query_results=10,\n",
    "    filtered=True,\n",
    "    filterer=filterer,\n",
    ")\n",
    "\n",
    "print(\"NN Negative Sampler\")\n",
    "print(sampler.sample(dataset.training.mapped_triples[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6937a186",
   "metadata": {},
   "source": [
    "## Using the samplers in a full training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce7d43",
   "metadata": {},
   "source": [
    "Lets see how to use our new samplers in a standard pykeen training and evaluation pipeline, first, we register the new samplers in the pykeen namesspace, in order to address them only by their string name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f79ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sampler_resolver.register(element=CorruptNegativeSampler)\n",
    "filterer_resolver.register(element=NullPythonSetFilterer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47c1a9",
   "metadata": {},
   "source": [
    "Then, we can just use them as any other negative sampler, as a comparison, we show the same pipeline using the random sampler, already available in pykeen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a9a16d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No random seed is specified. Setting to 1809488467.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navis/.pyenv/versions/pykeen-venv/lib/python3.12/site-packages/pykeen/triples/triples_factory.py:740: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = dict(torch.load(path.joinpath(cls.base_file_name)))\n",
      "/Users/navis/.pyenv/versions/pykeen-venv/lib/python3.12/site-packages/pykeen/triples/triples_factory.py:740: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = dict(torch.load(path.joinpath(cls.base_file_name)))\n",
      "/Users/navis/.pyenv/versions/pykeen-venv/lib/python3.12/site-packages/pykeen/triples/triples_factory.py:740: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = dict(torch.load(path.joinpath(cls.base_file_name)))\n",
      "/Users/navis/.pyenv/versions/pykeen-venv/lib/python3.12/site-packages/pykeen/datasets/base.py:263: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  metadata = torch.load(metadata_path) if metadata_path.is_file() else None\n",
      "No cuda devices were available. The model runs on CPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bde50dc5a447daa2d91425770745e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cpu:   0%|          | 0/5 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95c4e107a484e8e9518e2227fdc71b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c54da6a6fe4427b026486e14529b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48865c090a184b029e0eea7e9553d850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb5e6338424427ab2088783b972e344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fe86be1255431b9e4e205282a8e54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92030568e4f74b109949695ce59019cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cpu:   0%|          | 0.00/201 [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.06s seconds\n",
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 2017647345.\n",
      "INFO:pykeen.datasets.utils:Loading cached preprocessed dataset from file:///Users/navis/.data/pykeen/datasets/nations/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM\n",
      "INFO:pykeen.triples.triples_factory:Loading from file:///Users/navis/.data/pykeen/datasets/nations/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM/training\n",
      "/Users/navis/.pyenv/versions/pykeen-venv/lib/python3.12/site-packages/pykeen/triples/triples_factory.py:740: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = dict(torch.load(path.joinpath(cls.base_file_name)))\n",
      "INFO:pykeen.triples.triples_factory:Loading from file:///Users/navis/.data/pykeen/datasets/nations/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM/testing\n",
      "/Users/navis/.pyenv/versions/pykeen-venv/lib/python3.12/site-packages/pykeen/triples/triples_factory.py:740: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = dict(torch.load(path.joinpath(cls.base_file_name)))\n",
      "INFO:pykeen.triples.triples_factory:Loading from file:///Users/navis/.data/pykeen/datasets/nations/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM/validation\n",
      "/Users/navis/.pyenv/versions/pykeen-venv/lib/python3.12/site-packages/pykeen/triples/triples_factory.py:740: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = dict(torch.load(path.joinpath(cls.base_file_name)))\n",
      "/Users/navis/.pyenv/versions/pykeen-venv/lib/python3.12/site-packages/pykeen/datasets/base.py:263: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  metadata = torch.load(metadata_path) if metadata_path.is_file() else None\n",
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "INFO:pykeen.pipeline.api:Using device: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb0a53bfc394e09afc3638f2c2c8e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cpu:   0%|          | 0/5 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d32be93e1944a4f92ffd3329d94b7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c631fecca1546998953d869e71d5bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9334ad103648e9a42bb4cd680e512e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140e03c65e3e4a40b4cee957ba4e49f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1f386e557d4cafadd025cbc5f81a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c999aae8d00540e4b32bface127722e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cpu:   0%|          | 0.00/201 [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.03s seconds\n"
     ]
    }
   ],
   "source": [
    "# NEW SAMPLER\n",
    "\n",
    "pipeline_result = pipeline(\n",
    "    dataset=\"Nations\",\n",
    "    model=\"TransE\",\n",
    "    negative_sampler=\"corrupt\",\n",
    "    negative_sampler_kwargs=dict(filtered=True, filterer=\"nullpythonset\"),\n",
    "    training_loop=\"sLCWA\",\n",
    ")\n",
    "\n",
    "# RANDOM SAMPLER\n",
    "\n",
    "pipeline_result = pipeline(\n",
    "    dataset=\"Nations\",\n",
    "    model=\"TransE\",\n",
    "    negative_sampler=\"basic\",\n",
    "    negative_sampler_kwargs=dict(filtered=True, filterer=\"bloom\"),\n",
    "    training_loop=\"sLCWA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ead1b",
   "metadata": {},
   "source": [
    "As you can see, the two pipeline are excatly the same, and given the corret kwargs with the additional needed information, the new samplers can be integrated in any pre-existing pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b49a6",
   "metadata": {},
   "source": [
    "## Writing your own custom subset based negative samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540c4c9",
   "metadata": {},
   "source": [
    "Lets assume we want to create a negative sampler that uses our proposed abstraction to corrupt triple with this rationale:\n",
    "For each triple to be corrupted, given its relation, use as a negative pool the top X less occuring entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f41190bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[    0,     1,   939],\n",
      "         [    0,     1,   939],\n",
      "         [    0,     1,   939],\n",
      "         [84088,     1,  5226],\n",
      "         [94074,     1,  5226]],\n",
      "\n",
      "        [[    0,     1,   939],\n",
      "         [ 4040,     1, 19014],\n",
      "         [94074,     1, 19014],\n",
      "         [    0,     1, 32666],\n",
      "         [    0,     1, 96783]]]), None)\n"
     ]
    }
   ],
   "source": [
    "class TutorialSampler(SubSetNegativeSampler):\n",
    "    def __init__(self, *args, top_k=100, **kwargs):\n",
    "        # We define this variable before super, to it can be available in the subset generation\n",
    "        object.__setattr__(self, \"top_k\", top_k)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    # Precompute the entity set for head and tail for each relation\n",
    "    def generate_subset(self, mapped_triples, **kwargs):\n",
    "        subset = dict()\n",
    "\n",
    "        for r in range(self.num_relations):\n",
    "\n",
    "            subset[r] = {0: None, 2: None}\n",
    "\n",
    "            for target in [const.HEAD, const.TAIL]:\n",
    "                data, counts = torch.unique(\n",
    "                    self.mapped_triples[self.mapped_triples[:, const.REL] == r, target],\n",
    "                    return_counts=True,\n",
    "                )\n",
    "                ordered_data = data[torch.sort(counts, descending=False)[1]][\n",
    "                    : self.top_k\n",
    "                ]\n",
    "\n",
    "                subset[r][target] = ordered_data\n",
    "\n",
    "        return subset\n",
    "\n",
    "    # Now lets define the negative pool for each triple\n",
    "    def strategy_negative_pool(self, h, r, t, target):\n",
    "        return self.subset[r][const.TARGET_TO_INDEX[target]]\n",
    "\n",
    "\n",
    "# Instantiate the sampler like any other one, adding our additional variable\n",
    "sampler = TutorialSampler(\n",
    "    mapped_triples=dataset.training.mapped_triples, top_k=5, num_negs_per_pos=5\n",
    ")\n",
    "\n",
    "# And just get the negatives!\n",
    "print(sampler.sample(dataset.training.mapped_triples[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d9cc09",
   "metadata": {},
   "source": [
    "As you can see, its extremely easy to define new negative samplers with our abstraction. Another detail is the use of integration, for example, with a top_k of 5, each triple can have a negative pool of 5 elements, this can be detrimental when the number of negative per positive are very high, in this case the \"integrate\" parameter can be used, and the negative pool will be integrated with addiotional entities sampled at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f55e76d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[    0,     1,  8268],\n",
      "         [    0,     1,   939],\n",
      "         [    0,     1, 96783],\n",
      "         [75891,     1,  5226],\n",
      "         [    0,     1,  1096],\n",
      "         [    0,     1, 32666],\n",
      "         [    0,     1, 85280],\n",
      "         [94074,     1,  5226],\n",
      "         [84088,     1,  5226],\n",
      "         [31776,     1,  5226],\n",
      "         [    0,     1, 64519],\n",
      "         [ 4040,     1,  5226],\n",
      "         [    0,     1, 28806],\n",
      "         [    0,     1, 21298],\n",
      "         [42033,     1,  5226],\n",
      "         [    0,     1, 90941],\n",
      "         [10327,     1,  5226],\n",
      "         [    0,     1, 75271],\n",
      "         [28065,     1,  5226],\n",
      "         [    0,     1, 71786],\n",
      "         [30858,     1,  5226],\n",
      "         [ 2295,     1,  5226],\n",
      "         [    0,     1, 53625],\n",
      "         [    0,     1, 90237],\n",
      "         [    0,     1, 71840],\n",
      "         [66410,     1,  5226],\n",
      "         [    0,     1, 38254],\n",
      "         [    0,     1, 44567],\n",
      "         [17956,     1,  5226],\n",
      "         [69772,     1,  5226],\n",
      "         [ 3414,     1,  5226],\n",
      "         [66616,     1,  5226],\n",
      "         [90105,     1,  5226],\n",
      "         [    0,     1,  5812],\n",
      "         [    0,     1, 86462],\n",
      "         [23140,     1,  5226],\n",
      "         [    0,     1, 59013],\n",
      "         [75136,     1,  5226],\n",
      "         [    0,     1, 53517],\n",
      "         [76700,     1,  5226],\n",
      "         [    0,     1, 49082],\n",
      "         [    0,     1, 39241],\n",
      "         [    0,     1, 78680],\n",
      "         [51923,     1,  5226],\n",
      "         [38448,     1,  5226],\n",
      "         [50054,     1,  5226],\n",
      "         [74491,     1,  5226],\n",
      "         [ 6448,     1,  5226],\n",
      "         [    0,     1, 20093],\n",
      "         [    0,     1,  6613],\n",
      "         [    0,     1, 58980],\n",
      "         [66546,     1,  5226],\n",
      "         [    0,     1, 36378],\n",
      "         [    0,     1, 43231],\n",
      "         [31780,     1,  5226],\n",
      "         [    0,     1, 49559],\n",
      "         [    0,     1, 47543],\n",
      "         [    0,     1, 51888],\n",
      "         [49195,     1,  5226],\n",
      "         [43364,     1,  5226],\n",
      "         [68419,     1,  5226],\n",
      "         [66179,     1,  5226],\n",
      "         [88246,     1,  5226],\n",
      "         [47141,     1,  5226],\n",
      "         [68608,     1,  5226],\n",
      "         [67259,     1,  5226],\n",
      "         [    0,     1, 15994],\n",
      "         [    0,     1, 81976],\n",
      "         [38776,     1,  5226],\n",
      "         [    0,     1, 83691],\n",
      "         [50831,     1,  5226],\n",
      "         [    0,     1, 67806],\n",
      "         [    0,     1, 79681],\n",
      "         [40093,     1,  5226],\n",
      "         [    0,     1, 48326],\n",
      "         [    0,     1, 25797],\n",
      "         [28367,     1,  5226],\n",
      "         [ 1274,     1,  5226],\n",
      "         [    0,     1, 29677],\n",
      "         [    0,     1,  9576],\n",
      "         [29009,     1,  5226],\n",
      "         [69013,     1,  5226],\n",
      "         [    0,     1, 15170],\n",
      "         [35303,     1,  5226],\n",
      "         [    0,     1, 81353],\n",
      "         [    0,     1, 82315],\n",
      "         [41368,     1,  5226],\n",
      "         [    0,     1, 28013],\n",
      "         [    0,     1, 51041],\n",
      "         [    0,     1, 29890],\n",
      "         [ 1760,     1,  5226],\n",
      "         [21281,     1,  5226],\n",
      "         [    0,     1,  2270],\n",
      "         [    0,     1, 22403],\n",
      "         [87641,     1,  5226],\n",
      "         [    0,     1, 32162],\n",
      "         [86162,     1,  5226],\n",
      "         [48045,     1,  5226],\n",
      "         [54054,     1,  5226],\n",
      "         [ 4582,     1,  5226]],\n",
      "\n",
      "        [[75891,     1, 19014],\n",
      "         [94074,     1, 19014],\n",
      "         [84088,     1, 19014],\n",
      "         [    0,     1,  8268],\n",
      "         [    0,     1,   939],\n",
      "         [31776,     1, 19014],\n",
      "         [    0,     1, 96783],\n",
      "         [    0,     1,  1096],\n",
      "         [    0,     1, 32666],\n",
      "         [    0,     1,  1761],\n",
      "         [ 4040,     1, 19014],\n",
      "         [    0,     1, 53368],\n",
      "         [58107,     1, 19014],\n",
      "         [    0,     1, 89991],\n",
      "         [    0,     1,  7802],\n",
      "         [    0,     1, 70619],\n",
      "         [    0,     1, 89131],\n",
      "         [30972,     1, 19014],\n",
      "         [    0,     1, 73968],\n",
      "         [    0,     1, 69412],\n",
      "         [40441,     1, 19014],\n",
      "         [    0,     1,  2733],\n",
      "         [    0,     1, 33982],\n",
      "         [90630,     1, 19014],\n",
      "         [    0,     1, 52367],\n",
      "         [    0,     1, 24267],\n",
      "         [48077,     1, 19014],\n",
      "         [    0,     1, 80650],\n",
      "         [37718,     1, 19014],\n",
      "         [64826,     1, 19014],\n",
      "         [    0,     1, 12797],\n",
      "         [10542,     1, 19014],\n",
      "         [ 9453,     1, 19014],\n",
      "         [83718,     1, 19014],\n",
      "         [21291,     1, 19014],\n",
      "         [33465,     1, 19014],\n",
      "         [59885,     1, 19014],\n",
      "         [80546,     1, 19014],\n",
      "         [29576,     1, 19014],\n",
      "         [25225,     1, 19014],\n",
      "         [39799,     1, 19014],\n",
      "         [27238,     1, 19014],\n",
      "         [    0,     1, 35986],\n",
      "         [22686,     1, 19014],\n",
      "         [28090,     1, 19014],\n",
      "         [    0,     1, 30027],\n",
      "         [    0,     1, 91298],\n",
      "         [92522,     1, 19014],\n",
      "         [    0,     1, 34646],\n",
      "         [    0,     1, 26796],\n",
      "         [15471,     1, 19014],\n",
      "         [    0,     1, 47351],\n",
      "         [    0,     1, 23930],\n",
      "         [23649,     1, 19014],\n",
      "         [ 6360,     1, 19014],\n",
      "         [75618,     1, 19014],\n",
      "         [89726,     1, 19014],\n",
      "         [    0,     1, 39967],\n",
      "         [28365,     1, 19014],\n",
      "         [44851,     1, 19014],\n",
      "         [65716,     1, 19014],\n",
      "         [95080,     1, 19014],\n",
      "         [    0,     1, 26081],\n",
      "         [    0,     1,  5884],\n",
      "         [    0,     1, 34520],\n",
      "         [83036,     1, 19014],\n",
      "         [    0,     1, 43259],\n",
      "         [    0,     1, 94530],\n",
      "         [    0,     1, 48287],\n",
      "         [12667,     1, 19014],\n",
      "         [    0,     1, 78567],\n",
      "         [ 5196,     1, 19014],\n",
      "         [    0,     1, 22758],\n",
      "         [    0,     1, 79158],\n",
      "         [39707,     1, 19014],\n",
      "         [24350,     1, 19014],\n",
      "         [96375,     1, 19014],\n",
      "         [    0,     1, 84114],\n",
      "         [    0,     1, 96340],\n",
      "         [    0,     1,  7654],\n",
      "         [    0,     1, 89936],\n",
      "         [76026,     1, 19014],\n",
      "         [59694,     1, 19014],\n",
      "         [53682,     1, 19014],\n",
      "         [70136,     1, 19014],\n",
      "         [    0,     1, 25615],\n",
      "         [    0,     1, 36490],\n",
      "         [    0,     1, 18384],\n",
      "         [    0,     1, 15861],\n",
      "         [    0,     1, 18004],\n",
      "         [13341,     1, 19014],\n",
      "         [91239,     1, 19014],\n",
      "         [94646,     1, 19014],\n",
      "         [    0,     1, 39183],\n",
      "         [    0,     1, 27446],\n",
      "         [    0,     1, 62787],\n",
      "         [    0,     1,  8953],\n",
      "         [33309,     1, 19014],\n",
      "         [37092,     1, 19014],\n",
      "         [    0,     1, 85676]]]), None)\n"
     ]
    }
   ],
   "source": [
    "sampler = TutorialSampler(\n",
    "    mapped_triples=dataset.training.mapped_triples,\n",
    "    top_k=5,\n",
    "    num_negs_per_pos=100,\n",
    "    integrate=True,\n",
    ")\n",
    "\n",
    "print(sampler.sample(dataset.training.mapped_triples[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6140d",
   "metadata": {},
   "source": [
    "Additionaly you can compute the dataset statistics directly using our provided functions, this can take some time, since this computation as to be computed for each `<h,r,*>` and `<*,r,t>` combination, for this reason we test it on a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d06b41d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SubsetNegativeSampler] Computing <h,r,*> Negative Pools\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4875/4875 [00:03<00:00, 1497.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SubsetNegativeSampler] Computing <*,r,t> Negative Pools\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3547/3547 [00:02<00:00, 1382.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " {0: (1, 0.00011873664212776063),\n",
       "  2: (4, 0.0004749465685110425),\n",
       "  10: (8422, 1.0),\n",
       "  40: (8422, 1.0),\n",
       "  100: (8422, 1.0)})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = TutorialSampler(\n",
    "    mapped_triples=dataset.training.mapped_triples, top_k=5, num_negs_per_pos=5\n",
    ")\n",
    "\n",
    "triples = dataset.training.mapped_triples\n",
    "sampler.average_pool_size(triples[torch.randperm(len(triples))][:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe44b87",
   "metadata": {},
   "source": [
    "The function produces the average number of entities in each negative pool (checking if there are false negative), and then in order, the number of triples that have less than 0, 2, 10, 40, 100 entities in their negative pool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykeen-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
